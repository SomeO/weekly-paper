# 目标检测
---
## 目标检测发展历史
### 一、Two stage object detection method
发展过程：R-CNN --> SPP --> Fast R-CNN --> Faster R-CNN
#### 1. R-CNN
![R-CNN网络结构](assets/markdown-img-paste-2018081410464459.png)
> 算法描述：
> 1. selective search方法选取2000个候选框proposal，
> 2. 每个候选框经过裁剪输出相同size的图片，并输入深度网络提取特征
> 3. 将特征输入到每一类所对应的SVM中，进行类别的分类
> 4. 并使用回归修正检测框的位置
>
> 优点：
>  深度学习方向目标检测的开山鼻祖
>
> 缺点：
> 1. selective search方法筛选proposal的量大，
> 2. 对所有的特征提取会有重复计算，处理速度非常慢


#### 2. SPP
![SPP的网络结构](assets/markdown-img-paste-20180814135339527.png)
> 算法描述：
> 针对R-CNN对图片进行crop再输入到CNN中可能会造成图片失真的缺点，做了改进
> 1. 添加了SPP（spatial pyramid Pooling）层，使得对于任意尺寸的输入，输出都是固定的size
> 2. 对原图进行一次卷积提取feature map，然后找到proposal在feature map对应的patch，将此patch作为后续网络的输入
> ![两种网络结构对比结果](assets/markdown-img-paste-20180814163608339.png)
>
> 优点(相比R-CNN):
> 1. 只进行了一次卷积操作，大大节省了处理proposal的时间，速度提高很多
> 2. 可以接收任意大小的输入
>
> 缺点:
> 1. 并未实现端到端的检测,仍然需要多阶段的训练
#### 3. Fast R-CNN
![Fast R-CNN网络结构](assets/markdown-img-paste-20180814151130725.png)
> 算法描述：
> 针对R-CNN和SPPnet做了改进
> 1. 通过SS方法生成一系列proposal方法
> 2. proposal和image输入卷积层
> 3. 对卷积层的输出做RoI Pooling操作，使得输出尺寸固定
> 4. 将RoI Pooling层的处理结果过进行分类和对检测框进行回归
>
> 优点:
> 1. 实现了大部分的end-to-end的训练
> 2. 提出了RoI Pooling操作，获得固定尺寸的输出
>
> 缺点：
> 1. proposal的筛选仍然不是很高效
#### 4. Faster R-CNN
![faster rcnn 网络结构图](assets/markdown-img-paste-20180811185555384.png)
> 算法描述：
> 针对Fast R-CNN方法进行了改进
> 1. 将image经过卷积层提取feature map
> 2. 再生成anchors，输入到RPN网络分别对每个anchor做fg/bg的分类，以及对bbox的回归
> 3. 根据RPN输出的anchors的预测值挑选出256个anchor作为最终的rois（proposal）
> 4. 通过RoI Pooling将挑选出的rois调整至固定的尺寸,再输入到Fast R-CNN进行分类和回归
>
> 优点：
> 1. 使用生成的anchors代替SS挑选的proposal，并且经过一次卷积，可以让后续的网络结构共享卷积
> 2. 提出RPN网络，判断anchors是fg/bg，以及对bbox作初步的回归
>
> 缺点：
> 1. 速度没有one stage检测算法快

### 二、One stage object detection method
参考博客：[An overview of object detection: one-stage methods.](https://www.jeremyjordan.me/object-detection-one-stage/)
发展过程：YOLO --> YOLOv2 --> SSD --> YOLOv3
#### 1. YOLO
![YOLO算法网络结构](assets/markdown-img-paste-20180818154228721.png)
> 参考：[YOLO--google幻灯片](https://docs.google.com/presentation/d/1aeRvtKG21KHdD5lg6Hgyhx5rPq_ZOsGjG5rJ1HP7BbA/pub?start=false&loop=false&delayms=3000&slide=id.g137784ab86_4_1079)
> 算法描述：
> 针对two-stage结构的目标检测算法，真正做到了端到端的检测，在速度上大幅提高
> 1. 将一幅图像分成SxS个网格(grid cell)，如果某个object的重心落在这个网络中，则这个网格就负责预测这个object
> 2. 每个网格预测B个bounding box（bbox）和一个confidence值，该confidence值通过下边公式计算，
> ![confidence计算公式](assets/markdown-img-paste-2018081816200128.png)
> 其中如果有Object落在一个grid cell里，则第一项Pr(Object)取1，否则取0；第二项是预测bbox和实际的ground truth的IoU值。同时每个网格还要预测所有类的类别信息（class信息），记为C，则对于每张图片要预测S x S x（5*B + C）这样一个输出，如下图：
> ![预测输出尺寸](assets/markdown-img-paste-20180818162826463.png)
> 3. 每个网格预测的class信息和bbox预测的confidence信息相乘，就得到每个bbox的class-specific confidence score:
> ![](assets/markdown-img-paste-20180818164444976.png)
> 等式左边第一项就是每个网格预测的类别信息，第二三项就是每个bbox预测的confidence。这个乘积即为预测的box属于某一类的概率。
> 4. 然后设置阈值过滤掉得分比较低的bboxes，通过NMS处理bbox得到最终的结果
>
> 优点：
> 1. 可以进行端到端的训练，检测速度快
> 2. 可以降低将背景检测为物体的概率
>
> 缺点：
> 1. 位置精确性差，对小目标物体及密集物体的检测不好
> 2. 召回率低
#### 2. SSD
SSD和YOLO网络结构的对比
> ![SSD和YOLO的网络结构对比](assets/markdown-img-paste-20180819173243833.png)
> 可以看到YOLO在卷积层后接全连接层，即检测时只利用了最高层feature maps（包括Faster RCNN也是如此）；而SSD采用了特征金字塔结构进行检测，即检测时利用了conv4-3，conv-7（FC7），conv6-2，conv7-2，conv8_2，conv9_2这些大小不同的feature maps，在多个feature maps上同时进行softmax分类和位置回归。<font color=#DC143C size=4 >SSD使用低层feature map检测小目标，使用高层feature map检测大目标。</font>
>
> 算法描述：
> 1. 将VGG16的两个全连接层改成卷积层，再增加4个卷积层构造网络结构
> 2. 对其中不同的卷积层的输出分别用两个3*3的卷积核进行卷积，一个输出分类用的confidence，一个输出用于回归用的bounding box
> 3. 针对上述的卷积层的feature map上生成一系列不同大小的default box
> 4. 得到不同feature map上的default box的偏移量以及得分后，通过NMS得到最终结果
>
> 检测结果对比
> ![作用在Pascal VOC2007上的结果](assets/markdown-img-paste-20180819190449625.png)
>
> 优点：
> 1. 对于不同纵横比的物体的检测都有效
>
> 缺点：
> 1. 对于小物体的检测比大物体的检测要差，增加输入图像的尺寸有利于小物体的检测
>
#### 3. YOLO9000
> ![YOLO9000网络结构Darknet-19](assets/markdown-img-paste-20180819153659542.png)
> 算法描述：
> 在YOLOv1的基础上提出了上图所示的Darknet-19网络结构，并进行了如下的改进
> 1. Batch Normalization  批量正则化，避免网络的每一层都在学习数据的新的分布，同时也有更好的泛化能力，mAP提升2%
> 2. High Resolution Classification 作者首先对分类网络（上述darknet-19）进行了fine tune，分辨率改成448 * 448，在ImageNet数据集上训练10个 epochs。然后，对检测网络部分也进行fine tune。通过提升输入的分辨率，mAP获得了4%的提升。
> 3. Convolution with Anchor box YOLO利用全连接层预测边框，导致空间信息丢失，定位不准，因此作者采用faster rcnn中的anchor box的思想，用anchor box来预测bounding box，mAP从69.5%下降到69.2%，但是召回率recall从81%上升到了88%
> 4. Dimension Clusters 维度聚类，在faster rcnn中，anchor box为先验框，训练过程中虽然也可以调整，但是作者更希望一开始就选择更具代表性的anchor，这样网络就更容易学到准确的预测位置，作者使用改进的K-Means方法对bounding box进行聚类，此处的改进主要是避免原K-Means方法对大box的error比小box的大，聚类结果会产生偏差，因此作者将评判标准改为了IoU，这样error就与box的大小无关。
> 5. Direct location prediction 直接位置预测，与faster rcnn中的bounding box regression预测偏移量不同，此处作者直接预测位置信息，与4中的维度聚类共同使得mAP提升了5%。
> 6. Fine-Grained Features 细粒度特征，通过把浅层特征图和深层特征图融合，提升对小物体检测
> 7. Multi-Scale Training 作者通过迭代10个epoch后，选择新的图片尺寸，以32的倍数为图片尺度池化，以此来提升网络的鲁棒性。
> 下图为应用上述7种方法后相对于YOLO的提升
> ![与YOLO对比的结果](assets/markdown-img-paste-20180819164728518.png)
> YOLOv2与其他检测算法的对比
> ![检测算法的对比](assets/markdown-img-paste-20180819165950144.png)
> ![检测算法的对比](assets/markdown-img-paste-20180819164552197.png)
>
> YOLO 9000 的网络结构与YOLOv2的是相同的，它允许实时地检测超过9000种物体分类，使用联合优化技术同时在ImageNet和COCO数据集上进行训练。
>
> 优点：
> 1. 检测速度更快
> 2. 泛化能力提升
>
> 缺点：
> 1. 作用在衣服或者设备上时，作用效果一般，因为COCO没有任何衣服之类的标签
> 2. 对小物体的检测有待提升

#### 4. YOLOv3
> YOLOv3论文中提出的Darknet-53网络结构
> ![YOLOv3网络结构图](assets/markdown-img-paste-20180819191717896.png)
> 算法描述：
> YOLOv3是在YOLOv2的基础上进行改进的，主要有以下几点：
> 1. 仍然使用YOLOv2的改进的K-Means方法进行聚类筛选anchor，不再使用softmax进行预测一个物体的得分，而是使用逻辑回归，进而可以处理重叠的多标签问题
> 2. 多尺度预测，对于不同尺度预测3个box，网络结构如下
> ![YOLOv3](assets/markdown-img-paste-20180819194901463.png)
>
> 优点：
> 1. 快速、准确率高
> 2. 泛化能力强
> 3. 提升了对小物体的检测
>
> 缺点：
> 对中等或者大物体的检测表现相对不好
>

---
## Faster R-CNN网络结构图
---
整个Faster R-CNN主要包括以下三个部分：
![](assets/markdown-img-paste-20180813002930167.png)
本次分享将会从一下几个方面展开：
1. 数据准备
2. Conv_layer
3. <font color=#DC143C size=4 >rpn_network（rpn_cls）</font>
4. <font color=#DC143C size=4 >bounding box regression（rpn_reg）</font>
5. <font color=#DC143C size=4 >proposal layer</font>
6. <font color=#DC143C size=4 >nms</font>
7. <font color=#DC143C size=4 >RoI Pooling</font>
8. fast rcnn (cls + reg)


---
## Faster R-CNN算法实现细节
---
本次分享的内容参考了以下内容：
1. [Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497.pdf)
2. [ An Implementation of Faster RCNN with Study for Region Sampling](https://arxiv.org/pdf/1702.02138.pdf)
3. [Tensorflow版本的Faster R-CNN实现](https://github.com/endernewton/tf-faster-rcnn)
4. [弄懂目标检测（Faster R-CNN）？看这篇就够了！](http://pancakeawesome.ink/%E5%BC%84%E6%87%82%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B(Faster-R-CNN))
### 一、数据准备
数据部分imdb和roidb通过combine_roidb()方法生成，数据格式如下
#### 1. imdb 图片数据
> **imdb**:
![imdb内部结构](assets/markdown-img-paste-2018081212444571.png)
#### 2. roidb roi数据
> **roidb[i]**
![roidb的内部结构](assets/markdown-img-paste-20180812154802979.png)
#### 3. config 配置参数
该部分主要包括config.py文件中定义的各种参数常量，如生成anchor的ratio、scale，nms中的阈值，以及proposal的数量等等常量
### 二、Conv_layer
![Conv_layer层示意图](assets/markdown-img-paste-20180812170943676.png)
作用：
> 生成大小为((M/16) , (N/16))的feature map

实现细节：
> 主要通过经典网络的前一部分结构生成feature map，该部分提供了以下网络结构
> 1. VGG16
> 2. Resnet50
> 3. Resnet101

注意事项：
> 1. 该部分涉及到卷积的操作都为Same卷积，即filter:3x3,stride:1,padding:1的卷积
> 2. 该部分的Pooling操作都为输出尺寸为输入一半的操作，即stride:2的pooling，共4次

### 三、Region Proposal Network (RPN网络)
![RPN网络结构](assets/markdown-img-paste-20180812172958950.png)
作用：
> 1. 根据feature map生成anchors
> 2. 判断每个anchors为前景还是背景
> 3. 输出anchors和真实groud truth的偏差值
> 4. 通过nms选出适当数量的anchors作为proposal

通过Conv_layer层将裁剪后的MxN的图片经过卷积、Pooling等操作，输出(M/16)x(N/16)大小的feature map(每个像素点512-d，代码中的是256-d)，针对feature map的每个像素点生成9个anchors，再对feature map进行3x3的卷积，然后输入到cls和reg网络分支中。

该部分主要分为以下四个部分：
#### 1. generate anchors
![所有anchors](assets/markdown-img-paste-20180812201456416.png)
作用：
> 生成 W * H * 9 个anchors (其中W,H分别为feature map的width和height，若原图M=800，N=600，则W=800/16=50，H=600/16=38，共生成17100个anchors)

实现细节：
> 生成anchors的方法：network._ anchor_component()
> 生成anchors的过程是根据原图尺寸生成的，主要步骤：
> 1. 将原图的width和height缩放feat_stride(实现过程中是16) 倍
> 2. 生成range(width) * feat_stride  和 range(height) * feat_stride，即width和height为关于16的等差数列 [0, 16, 32, 48, 64 ......]，并将两数列合并成[W, H, W, H]的二维数组，该二维数组刚好是将整个原图的width 和height 按照每等分16进行切分后 的所有16x16的像素块相对 ( 0, 0, 15, 15 ) 的坐标，记为shifts
> 3. 根据原图中的base_anchor (0, 0, 15, 15)生成关于第一个16x16像素块的一组共9 个anchors ( 根据scale：[8, 16, 32]，ratio：[0.5, 1, 2] 生成)
> 4. 然后将生成的anchors 和 shifts相加，结果就为所有16x16像素块所生成的anchors， 其中每个16x16的像素块生成9个anchors，共生成 ( width/16 ) * ( height/16 ) * 9 个anchors，每个anchor表示为（x1，y1，x2，y2），其中（x1，y1）和（x2，y2）分别表示左上角和右下角的坐标值（记图片的左上角为（0，0）点）。

#### 2. cls分支
作用：
>该分支对生成的anchor进行fg和bg的二分类预测

输入：
> 经过3*3的卷积后feature map

输出：
> rpn_cls_prob, 每个anchor是fg还是bg,fg为1，bg为0，

实现细节：

> 1. 对经过3x3卷积的feature map做1x1的卷积，channel为18（2x9）, 输出shape为[1, W, H, 18]的数据，
> 2. 将1的结果reshape成softmax层可以做二分类的形状[1, 2, W, H*9],然后再输入到softmax中，再将输出的[1, 2, W, Hx9]再次reshape回[1, w, H, 18]，输出即为对每个anchor的预测值rpn_cls_prob。

#### 3. reg分支
作用：
> 该分支对生成的anchor的bounding box做regression，输出bounding box的dx，dy，dw，dh四个量

输入：
> 经过3*3的卷积后feature map

输出：
> rpn_bbox_pred, 作为每个anchor的偏移量

实现细节：
> 对经过3x3卷积的feature map做1x1的卷积，channel为36（4x9）, 输出shape为[1, W, H, 36]的数据，将输出的rpn_bbox_pred输入给proposal layer。

#### 4. Proposal layer

##### 4.1. 详解 proposal_layer
作用：
> 挑选出置信度最高的2000个调整后的anchors作为rois， 以及对应的rpn_cls_prod作为roi_scores

输入：
> rpn_cls_prob, rpn_bbox_pred, im_info

输出：
> rois(2000), roi_scores(2000个rois对应的rpn_cls_prob)

实现细节：
> 1. 根据reg分支预测的rpn_bbox_pred对所有的anchors进行调整，使其更接近ground truth，调整策略如下：
> ![调整策略](assets/markdown-img-paste-20180812212427766.png)
> 2. 调整proposal中超出image的boxes的边界值，使其边界不超过image
> 3. 根据rpn_cls_prob和proposal使用非极大值抑制挑选出post_nms_topN(2000)个proposal
> 4. 挑选出非极大值抑制所选出的post_nms_topN个proposal的坐标以及所对应的scores
> 5. 给proposal的第一列插入0，并与scores一同返回，作为rois和rois_scores，此处添加的一列0，后续将作为每个roi的分类class

---
> **非极大值抑制(Non-maximal suppression,NMS)：**
> 1. 将所有框的得分排序，选中最高分及其对应的框
> 2. 遍历其余的框，如果和当前最高分框的重叠面积(IOU)大于一定阈值，我们就将框删除
> 3. 从未处理的框中继续选一个得分最高的，重复上述过程
> 非极大值抑制，挑选出符合条件的2000个boxes，并返回对应框的index
---
##### 4.2. 详解 anchor_target_layer
作用：
> 生成所有anchors所对应的rpn_labels

输入：
> rpn_cls_score, anchor

输出：
> rpn_labels（rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights）

实现细节：
> 1. 筛选在图片边界以内的所有anchor，将index记录到inds_inside，并获取对应的anchor，记录到anchors中
> 2. 生成len(inds_inside)长度的labels，将元素全置为-1
> 3. 获取每一个anchor和gt_box的overlap，组合成为overlaps矩阵，gt_boxes代表列，anchors代表行；获取最大的前len(inds_inside)个overlap记录到max_overlaps中，获取最大的overlap，以及最大的overlap所对应的index，保存至gt_argmax_overlaps中

      anchors\gt_boxes| gt_1  | ...  | gt_m
      :--: |:---:|:---:|:---:|:--:|
      | anchor_1  | 0.5  | ... | 0.2
      |     ...   | ...  | ... | ...
      | anchor_n  | 0.7  | ... | 0.3
> 4. 判断overlap中与labels对应的index的元素，若该元素小于0.3，则将labels对应位置的元素置为0，若大于0.7以及等于最大的overlap，则将labels对应位置的元素置为1.
> 5. 对正负样本进行取样，样本总数为TRAIN.RPN_BATCHSIZE(256)，正样本的比例为TRAIN.RPN_FG_FRACTION(0.5),判断labels中，正样本的个数(np.sum(labels == 1))若大于128，则从正样本中随机选取(正样本总数-128)个正样本置为-1，负样本也是同样的操作。
> 6. 计算anchors和ground truth的target，即bounding boxes regression的目标函数:
      &nbsp;&nbsp;&nbsp;&nbsp; # 该值越小说明anchor和ground truth越接近
      &nbsp;&nbsp;&nbsp;&nbsp; targets_dx = (gt_ctr_x - ex_ctr_x) / ex_widths
      &nbsp;&nbsp;&nbsp;&nbsp; targets_dy = (gt_ctr_y - ex_ctr_y) / ex_heights
      &nbsp;&nbsp;&nbsp;&nbsp; # 若为0，则说明gt_widths和ex_widths比值为1，即相等
      &nbsp;&nbsp;&nbsp;&nbsp; targets_dw = np.log(gt_widths / ex_widths)
      &nbsp;&nbsp;&nbsp;&nbsp; targets_dh = np.log(gt_heights / ex_heights)
> 7. 生成全0的bbox_inside_weights数组，并将labels == 1的对应位置的元素赋值为[1.0, 1.0, 1.0, 1.0]，表示只计算fg的回归，并计算bbox_outside_weights。
> 8. 最后将labels、bbox_target、bbox_inside_weights以及bbox_outside_weights映射回feature map对应的各个值，并保存至self._ anchor_targets中，最后返回rpn_labels

##### 4.3. 详解 proposal_target_layer
作用：
> 该方法从2000个rois中人挑选出256个rois(256, 5)

输入：
> rois，rois_scores   （2000个rois和对应的rois_scores）

输出：
> rois （挑选出256个）

实现细节：
> 1. 对proposal_layer筛选出来的2000个rois，以及对应的scores做采样
> 2. 通过_sample_rois挑选出与gt_boxes的overlaps最大的所有rois，以及对应的gt_boxes的labels
> 3. 将overlaps大于0.5的rois作为fg，小于0.5大于0的作为bg
> 4. 确保所有的fg和bg之和为rois_per_image(256个rois)，并且每张图片的fg的数量不能超过fg_rois_per_image，剩下的全部为bg
> 5. 从labels中挑选出256个rois对应的分类，重新赋给labels，以及计算生成对应的bbox_targets_data
> 6. 将生成的bbox_target_data中的数据赋给对应的bbox_targets(256, 84)，其中84代表21*4，即每个rois可能有21个分类，每个分类对应4个bbox_targets数值

### 四、RoI Pooling
![RoI Pooling](assets/markdown-img-paste-20180812223218887.png)
作用：
>RoI pooling : proposal --> 14 x 14 --> 7 x 7

输入:
> bottom(net_conv, 即feature map)， rois(256个rois)

输出：
> resize后的大小为7x7的rois

实现细节：
> 1. 先将rois的第一列取出赋值给batch_ids
> 2. 标准化x1, y1, x2, y2使得输入的参数符合tf.image.crop_and_resize的要求
> 3. 通过crop_and_resize将筛选出的形状不一的rois裁剪成14x14的大小
> 4. 再通过最大池化将14x14的rois缩小为7x7的大小

### 五、fast rcnn
![fast rcnn](assets/markdown-img-paste-20180812223336927.png)
#### 1. cls
#### 2. reg
上述两个分支的作用和之前RPN网络的两个分支类似，都是分别分类和对bounding box的回归，但不一样的地方在于，此处的分类是针对所有类别进行分类，即共需要分为除去bg后的20类物体。


### 六、训练过程
#### 增加loss
> 1. RPN cls loss
> 2. RPN reg loss
> 3. RCNN cls loss
> 4. RCNN reg loss




















## 扩展内容
1. [L1和L2正则化的直观理解](https://blog.csdn.net/jinping_shi/article/details/52433975)
